---
title: "US Judge ratings"
output:
  pdf_document:
    latex_engine: xelatex
---
## Regression Analysis for ROCK
step 1. data preparation
step 2. variable transformation
step 3. building the model and variable selection
step 4. testing assumptions

```{r}
data <- rock
head(data)
```
\newpage
STEP 1. DATA PREPARATION
```{r}
# 1. Basic structure & missingness
cat("Missing values:", sum(is.na(data)), "\n")
cat("\nNo missing values -- ok to proceed with whole dataset")
```
\newpage
STEP 2. VARIABLE TRANSFORMATION

lets first plot the variables against the target, we don't see a clear linear relationship

```{r}
# Get predictor names (exclude 'perm')
predictors <- names(data)[names(data) != "perm"]

# Set up plotting layout
par(mfrow = c(2, 2))

# Loop through each predictor and create plots
for (predictor in predictors) {
  # Scatter plot with linear fit
  plot(data[[predictor]], data$perm, 
       xlab = predictor, ylab = "perm",
       main = paste("perm vs", predictor),
       pch = 16, col = "blue")

}

```
testing some e^-x or ln(x) doesn;t really help either

```{r}
# Test exponential decay transformation for peri
par(mfrow = c(2, 3))

# Original peri vs perm
plot(data$peri, data$perm, main = "peri vs perm (original)",
     xlab = "peri", ylab = "perm", pch = 16, col = "blue")

# Exponential decay transformation: perm ~ exp(-peri) or perm ~ 1/peri
plot(1/data$peri, data$perm, main = "perm vs 1/peri",
     xlab = "1/peri", ylab = "perm", pch = 16, col = "red")
abline(lm(perm ~ I(1/peri), data = data), col = "darkred", lwd = 2)

plot(exp(-data$peri), data$perm, main = "perm vs exp(-peri)",
     xlab = "exp(-peri)", ylab = "perm", pch = 16, col = "purple")
abline(lm(perm ~ I(exp(-peri)), data = data), col = "darkviolet", lwd = 2)

# Test logarithmic transformation for shape
# Original shape vs perm
plot(data$shape, data$perm, main = "shape vs perm (original)",
     xlab = "shape", ylab = "perm", pch = 16, col = "blue")

# Log transformation
plot(log(data$shape), data$perm, main = "perm vs log(shape)",
     xlab = "log(shape)", ylab = "perm", pch = 16, col = "red")
abline(lm(perm ~ I(log(shape)), data = data), col = "darkred", lwd = 2)

# Square root transformation (alternative)
plot(sqrt(data$shape), data$perm, main = "perm vs sqrt(shape)",
     xlab = "sqrt(shape)", ylab = "perm", pch = 16, col = "green")
abline(lm(perm ~ I(sqrt(shape)), data = data), col = "darkgreen", lwd = 2)

par(mfrow = c(1, 1))
```
Higher order terms don't really help make the relationship linear
```{r}
# Try alternative transformations
par(mfrow = c(2, 3))

# For peri - try polynomial and other transformations
plot(data$peri, data$perm, main = "peri vs perm (original)",
     xlab = "peri", ylab = "perm", pch = 16, col = "blue")

# Try polynomial for peri
plot(data$peri^2, data$perm, main = "perm vs peri^2",
     xlab = "peri^2", ylab = "perm", pch = 16, col = "red")

# Try sqrt for peri (since it might be scale-related)
plot(sqrt(data$peri), data$perm, main = "perm vs sqrt(peri)",
     xlab = "sqrt(peri)", ylab = "perm", pch = 16, col = "green")

# For shape - try polynomial and reciprocal
plot(data$shape, data$perm, main = "shape vs perm (original)",
     xlab = "shape", ylab = "perm", pch = 16, col = "blue")

# Try polynomial for shape
plot(data$shape^2, data$perm, main = "perm vs shape^2",
     xlab = "shape^2", ylab = "perm", pch = 16, col = "red")

# Try reciprocal for shape
plot(1/data$shape, data$perm, main = "perm vs 1/shape",
     xlab = "1/shape", ylab = "perm", pch = 16, col = "purple")

par(mfrow = c(1, 1))
```
```{r}
# Get predictor names (exclude 'perm')
predictors <- names(data)[names(data) != "perm"]

# Set up plotting layout: 2 plots per predictor
par(mfrow = c(2, 2))

# Loop through each predictor
for (predictor in predictors) {
  # Fit simple linear model
  formula <- as.formula(paste("perm ~", predictor))
  model <- lm(formula, data = data)
  
  # Calculate residuals and fitted values
  residuals <- resid(model)
  fitted <- fitted(model)
  
  # Plot 1: Residuals vs Fitted Values
  plot(fitted, residuals, 
       main = paste("Residuals vs Fitted:", predictor),
       xlab = "Fitted Values", ylab = "Residuals",
       pch = 16, col = "blue")
  abline(h = 0, col = "red", lwd = 2)
  
  # Plot 2: Q-Q Plot
  qqnorm(residuals, main = paste("Q-Q Plot:", predictor))
  qqline(residuals, col = "red", lwd = 2)
}

# Reset plotting layout
par(mfrow = c(1, 1))
```
There looks like non-normality in the data according to the QQ plot, lets test some transformations on the Y variable, `perm`
ln(perm) looks like a good transformation
```{r}
# You might want to check if transformations help:
# Try transforming the response variable 'perm'
par(mfrow = c(2, 2))

# Original model
model_original <- lm(perm ~ shape, data = data)
qqnorm(resid(model_original), main = "Original")
qqline(resid(model_original))

# Log transformation
model_log <- lm(log(perm) ~ shape, data = data)
qqnorm(resid(model_log), main = "Log(perm)")
qqline(resid(model_log))

# Square root transformation  
model_sqrt <- lm(sqrt(perm) ~ shape, data = data)
qqnorm(resid(model_sqrt), main = "Sqrt(perm)")
qqline(resid(model_sqrt))

# Box-Cox transformation (if needed)
# library(MASS)
# boxcox(perm ~ shape, data = data)
```
Lets plot `area` against the ln(perm) 
```{r}

data$log_perm <- log(data$perm)

# Set up plotting for area
par(mfrow = c(1, 3))

# Fit model with log(perm)
model_area <- lm(log_perm ~ area, data = data)
residuals <- resid(model_area)
fitted <- fitted(model_area)

# Plot 1: log(perm) vs area
plot(data$area, data$log_perm, 
     main = "log(perm) vs area",
     xlab = "area", ylab = "log(perm)",
     pch = 16, col = "blue")
abline(model_area, col = "red", lwd = 2)

# Plot 2: Residuals vs Fitted
plot(fitted, residuals, 
     main = "Residuals vs Fitted:\nlog(perm) ~ area",
     xlab = "Fitted Values", ylab = "Residuals",
     pch = 16, col = "darkgreen")
abline(h = 0, col = "red", lwd = 2)

# Plot 3: Q-Q Plot
qqnorm(residuals, main = "Q-Q Plot:\nlog(perm) ~ area")
qqline(residuals, col = "red", lwd = 2)

par(mfrow = c(1, 1))
```
Lets plot `peri` against ln(perm)
```{r}
# Set up plotting for peri
par(mfrow = c(1, 3))

# Fit model with log(perm)
model_peri <- lm(log_perm ~ peri, data = data)
residuals <- resid(model_peri)
fitted <- fitted(model_peri)

# Plot 1: log(perm) vs peri
plot(data$peri, data$log_perm, 
     main = "log(perm) vs peri",
     xlab = "peri", ylab = "log(perm)",
     pch = 16, col = "blue")
abline(model_peri, col = "red", lwd = 2)

# Plot 2: Residuals vs Fitted
plot(fitted, residuals, 
     main = "Residuals vs Fitted:\nlog(perm) ~ peri",
     xlab = "Fitted Values", ylab = "Residuals",
     pch = 16, col = "darkgreen")
abline(h = 0, col = "red", lwd = 2)

# Plot 3: Q-Q Plot
qqnorm(residuals, main = "Q-Q Plot:\nlog(perm) ~ peri")
qqline(residuals, col = "red", lwd = 2)

par(mfrow = c(1, 1))
```


Lets plot `shape` against ln(perm)
```{r}
# Set up plotting for shape
par(mfrow = c(1, 3))

# Fit model with log(perm)
model_shape <- lm(log_perm ~ shape, data = data)
residuals <- resid(model_shape)
fitted <- fitted(model_shape)

# Plot 1: log(perm) vs shape
plot(data$shape, data$log_perm, 
     main = "log(perm) vs shape",
     xlab = "shape", ylab = "log(perm)",
     pch = 16, col = "blue")
abline(model_shape, col = "red", lwd = 2)

# Plot 2: Residuals vs Fitted
plot(fitted, residuals, 
     main = "Residuals vs Fitted:\nlog(perm) ~ shape",
     xlab = "Fitted Values", ylab = "Residuals",
     pch = 16, col = "darkgreen")
abline(h = 0, col = "red", lwd = 2)

# Plot 3: Q-Q Plot
qqnorm(residuals, main = "Q-Q Plot:\nlog(perm) ~ shape")
qqline(residuals, col = "red", lwd = 2)

par(mfrow = c(1, 1))
```

`Shape` variable looks good, lets look at `peri` and `area`


`peri` seems to have a quadratic form with perm
```{r}
# Test polynomial relationships
par(mfrow = c(2, 2))

# Peri polynomials
plot(data$peri, data$log_perm, main = "log(perm) vs peri", 
     xlab = "peri", ylab = "log(perm)", pch = 16, col = "blue")
peri_quad <- lm(log_perm ~ poly(peri, 2), data = data)
x_seq <- seq(min(data$peri), max(data$peri), length = 100)
y_pred <- predict(peri_quad, newdata = data.frame(peri = x_seq))
lines(x_seq, y_pred, col = "red", lwd = 2)

# Area polynomials
plot(data$area, data$log_perm, main = "log(perm) vs area", 
     xlab = "area", ylab = "log(perm)", pch = 16, col = "blue")
area_quad <- lm(log_perm ~ poly(area, 2), data = data)
x_seq <- seq(min(data$area), max(data$area), length = 100)
y_pred <- predict(area_quad, newdata = data.frame(area = x_seq))
lines(x_seq, y_pred, col = "red", lwd = 2)

par(mfrow = c(1, 1))
```
\newpage
STEP 3. fitting the model - lets test our transformed variables against the standard linear model as a baseline

we see that the adjusted R^2 of the standard linear model is still higher than the transformed model despite the transformed model having a lower AIC and BIC. This suggests that the `area` variable has some explanatory power.

```{r}
# Model 1: Linear model with original perm
model1 <- lm(perm ~ peri + area + shape, data = data)

# Model 2: Log model with quadratic peri term
model2 <- lm(log(perm) ~ peri + I(peri^2) + shape, data = data)

# Calculate metrics for both models
results <- data.frame(
  Model = c("perm ~ peri + area + shape", "log(perm) ~ peri + peri^2 + shape"),
  R_squared = c(summary(model1)$r.squared, summary(model2)$r.squared),
  Adj_R_squared = c(summary(model1)$adj.r.squared, summary(model2)$adj.r.squared),
  AIC = c(AIC(model1), AIC(model2)),
  BIC = c(BIC(model1), BIC(model2))
)
results
```

F test to see if we need the area variable, we reject the null at the 0.1% level, suggesting that we need the area variable as well.

```{r}
# Reduced model (without area)
reduced_model <- lm(log(perm) ~ peri + I(peri^2) + shape, data = data)

# Full model (with area)
full_model <- lm(log(perm) ~ peri + I(peri^2) + area + shape, data = data)

# F-test to compare models
f_test <- anova(reduced_model, full_model)
print("=== F-test for including area ===")
print(f_test)
```

It seems like model 3 has the lowest AIC and BIC with the highest R^2
```{r}
# Model 1: Linear model with original perm
model1 <- lm(perm ~ peri + area + shape, data = data)

# Model 2: Log model with quadratic peri term
model2 <- lm(log(perm) ~ peri + I(peri^2) + shape, data = data)

# Model 3: Log model with quadratic peri term + area
model3 <- lm(log(perm) ~ peri + I(peri^2) + shape + area, data = data)

# Calculate metrics for all models
results <- data.frame(
  Model = c("perm ~ peri + area + shape", 
            "log(perm) ~ peri + peri^2 + shape", 
            "log(perm) ~ peri + peri^2 + shape + area"),  # Fixed the typo here
  R_squared = c(summary(model1)$r.squared, summary(model2)$r.squared, summary(model3)$r.squared),
  Adj_R_squared = c(summary(model1)$adj.r.squared, summary(model2)$adj.r.squared, summary(model3)$adj.r.squared),
  AIC = c(AIC(model1), AIC(model2), AIC(model3)),
  BIC = c(BIC(model1), BIC(model2), BIC(model3))
)

print(results)
```

final model selected 
```{r}
summary(model3)
```

STEP 4. testing assumptions -- residuals look pretty randomly scattered, and the QQ plot, while not completely hugging the line look decent. 

Residuals vs Fitted Plot (if it's random noise):
What it tells us:

Linearity assumption is satisfied - the relationship between predictors and response is properly captured

Constant variance (homoscedasticity) - the spread of residuals is consistent across all fitted values

No pattern means no systematic bias in the model

The model specification is correct - you've included the right terms (like the quadratic peri term)


Q-Q Plot (if it hugs the line):
What it tells us:

Normality assumption is satisfied - the residuals follow a normal distribution

Reliable p-values and confidence intervals - statistical inference is valid

No extreme outliers that could distort the results

The error distribution is well-behaved


```{r}
# Fit the best model
model3 <- lm(log(perm) ~ peri + I(peri^2) + shape + area, data = data)

# Set up plotting area
par(mfrow = c(1, 2))

# 1. Residuals vs Fitted (Linearity & Constant Variance)
plot(fitted(model3), resid(model3),
     main = "Residuals vs Fitted",
     xlab = "Fitted Values", ylab = "Residuals",
     pch = 16, col = "blue")
abline(h = 0, col = "red", lwd = 2)

# 2. Q-Q Plot (Normality)
qqnorm(resid(model3), main = "Q-Q Plot")
qqline(resid(model3), col = "red", lwd = 2)

par(mfrow = c(1, 1))
```
